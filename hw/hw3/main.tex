\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage[T1]{fontenc}

% macro to select a scaled-down version of Bera Mono (for instance)
\makeatletter
\newcommand\BeraMonottfamily{%
  \def\fvm@Scale{0.85}% scales the font down
  \fontfamily{fvm}\selectfont% selects the Bera Mono font
}
\makeatother

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\BeraMonottfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle,
        otherkeywords={True,False}
}

\title{CS249 Fall 2020\\
       Problem Set 3: Prediction}
\author{Christopher Munoz Cortes}
\date{\today}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Ridge Regression}

\begin{enumerate}[label={(\alph*)}]
    \item Write the objective function of a ridge regression in the matrix format.
    \[
        \text{RSS}(\beta) = (Y - X\beta)^T (Y - X\beta) + \lambda \beta^T \beta
    \]
    \item Compute the gradient of the objective function with respect to the vector of
    the coefficient in the model.
    
    Simplifying the objective function before taking the gradient yields:
    \begin{align*}
        \text{RSS}(\beta) &= (Y^T - \beta^T X^T) (Y - X\beta) + \lambda \beta^T \beta\\
        &= Y^T Y - \beta^T X^T Y - Y^T X\beta - \beta^T X^T X \beta + 
           \lambda \beta^T \beta\\
        &= Y^T Y -2\beta^TX^TY + \beta^T(X^TX)\beta + \lambda\beta^T\beta
    \end{align*}
    Now, taking the gradient with respect to $\beta$ gives:
    \[
        \nabla_\beta\,\text{RSS}(\beta) = -2X^TY + 2X^TX\beta + 2\lambda\beta
    \]
    \item Show that the solution to the ridge regression can be written in the following
    form:
    \[
        \hat{\beta} = (X^T X + \lambda I)^{-1} X^T Y
    \]
    where $X$ is the design matrix, $Y$ is the vector of the outcomes, and $\lambda$ is
    the regularization parameter.
    
    Setting the expression from part (b) equal to zero and solving for $\beta$:
    \begin{gather*}
        \nabla_\beta\,\text{RSS}(\beta) = 0\\
        -2X^TY + 2X^TX\beta + 2\lambda\beta = 0\\
        (X^TX + \lambda I) \beta = (Y^TX)^T\\
        \boxed{\hat{\beta}= (X^TX + \lambda I)^{-1}\, X^TY} \\
    \end{gather*}
    %\begin{equation*}
    %\end{equation*}
\end{enumerate}
\pagebreak

\section{Regression Models}
\begin{enumerate}[label={(\alph*)}]
    \item In many settings, our data contains variables with missing values.
    Search online and find suitable ways to impute missing values in a
    dataset. Use one of these methods to treat such variables in the data.
    
    The proper approach to handling missing values in a dataset depends on 
    \textit{missingness mechanism}, i.e. why the data are missing. 
    \item Name two categorical features in the data, and choose ten features from
    the dataset that you think will be most predictive of the outcome
    \begin{itemize}
        \item Two categorical features: \texttt{OverallCond}, \texttt{BldgType}
        \item Ten best predictors based on correlation: \texttt{SalePrice}, 
        \texttt{OverallQual}, \texttt{GrLivArea}, \texttt{GarageCars}, 
        \texttt{GarageArea}, \texttt{TotalBsmtSF}, \texttt{1stFlrSF},
        \texttt{FullBath}, \texttt{TotRmsAbvGrd}, \texttt{YearBuilt}, 
        \texttt{YearRemodAdd}
    \end{itemize}
    
    \item Transform the categorical features in your chosen set to make them
    suitable for modeling
    
    See code below.
    
    \item Apply ridge regression to the data and find the best value of the 
    regularization parameter using cross-validation. Report the RMSE of your best 
    model both in training and validation sets. Do you see any overfitting in your 
    model?


\end{enumerate}
\pagebreak

\section{Feature Selection}
\pagebreak

\section{Predicting The Election Outcome}
\begin{lstlisting}[language=Python, caption=Code for Question 1 \emph{Hypothesis
Testing}]

\end{lstlisting}

\pagebreak

\section{Regression to the Mean}

%\bibliographystyle{plain}
%\bibliography{references}
\end{document}