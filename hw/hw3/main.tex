\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage[T1]{fontenc}

% macro to select a scaled-down version of Bera Mono (for instance)
\makeatletter
\newcommand\BeraMonottfamily{%
  \def\fvm@Scale{0.85}% scales the font down
  \fontfamily{fvm}\selectfont% selects the Bera Mono font
}
\makeatother

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\BeraMonottfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle,
        otherkeywords={True,False}
}

\title{CS249 Fall 2020\\
       Problem Set 3: Prediction}
\author{Christopher Munoz Cortes}
\date{\today}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Ridge Regression}

\begin{enumerate}[label={(\alph*)}]
    \item Write the objective function of a ridge regression in the matrix format.
    \[
        \text{RSS}(\beta) = (Y - X\beta)^T (Y - X\beta) + \lambda \beta^T \beta
    \]
    \item Compute the gradient of the objective function with respect to the vector of
    the coefficient in the model.
    
    Simplifying the objective function before taking the gradient yields:
    \begin{align*}
        \text{RSS}(\beta) &= (Y^T - \beta^T X^T) (Y - X\beta) + \lambda \beta^T \beta\\
        &= Y^T Y - \beta^T X^T Y - Y^T X\beta - \beta^T X^T X \beta + 
           \lambda \beta^T \beta\\
        &= Y^T Y -2\beta^TX^TY + \beta^T(X^TX)\beta + \lambda\beta^T\beta
    \end{align*}
    Now, taking the gradient with respect to $\beta$ gives:
    \[
        \nabla_\beta\,\text{RSS}(\beta) = -2X^TY + 2X^TX\beta + 2\lambda\beta
    \]
    \item Show that the solution to the ridge regression can be written in the following
    form:
    \[
        \hat{\beta} = (X^T X + \lambda I)^{-1} X^T Y
    \]
    where $X$ is the design matrix, $Y$ is the vector of the outcomes, and $\lambda$ is
    the regularization parameter.
    
    Setting the expression from part (b) equal to zero and solving for $\beta$:
    \begin{gather*}
        \nabla_\beta\,\text{RSS}(\beta) = 0\\
        -2X^TY + 2X^TX\beta + 2\lambda\beta = 0\\
        (X^TX + \lambda I) \beta = (Y^TX)^T\\
        \boxed{\hat{\beta}= (X^TX + \lambda I)^{-1}\, X^TY} \\
    \end{gather*}
    %\begin{equation*}
    %\end{equation*}
\end{enumerate}
\pagebreak

\section{Regression Models}
\begin{enumerate}[label={(\alph*)}]
    \item In many settings, our data contains variables with missing values.
    Search online and find suitable ways to impute missing values in a
    dataset. Use one of these methods to treat such variables in the data.
    
    Some imputation methods are:
    \begin{itemize}
      \item \emph{Mean, median, mode imputation}. Replace each missing value with
      the mean, or the median, or the mode (most frequent value) of the observed 
      values for that feature.
      \item \emph{Regression imputation}. If we know there's a correlation between
      the missing value and other features, it is possible to get better estimates
      by running a linear regression for the missing feature values on other
      features.
      \item \emph{K-nearest Neighbor Imputation}. Neighbor-based imputation replaces
      the missing values with the most frequent value among the $k$-nearest neighbor
      for categorical data, and mean or mode for continuous variables.
      \item \emph{Multiple Imputation}. The single imputation methods above are limited
      in that they do not reflect the same variability from the sample data and the
      missing values. Multiple imputation methods create several imputed values for
      each missing value. Since each value is predicted from a slightly different model,
      it reflects sampling variability. One such method is MICE (Multivariate Imputation
      by Chained Equation).
    \end{itemize}
    
    For instance, if we were to take all the numerical features in our data and fill in
    missing values, we could use \texttt{scikit-learn}'s \texttt{IterativeImputer}.
    This implementation models each feature with missing values as a function of other
    features in a round-robin fashion.

\begin{lstlisting}[language=Python, caption=Imputing missing values using
\texttt{IterativeImputer}]
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Trainset is a dataframe with the data from `train.csv`
X = trainset.select_dtypes([np.number])
feat_names = X.columns
imp = IterativeImputer(max_iter=10, random_state=0, n_nearest_features=4)
X = pd.DataFrame(data=imp.fit_transform(X), columns=feat_names)
\end{lstlisting}
    
    \item Name two categorical features in the data, and choose ten features from
    the dataset that you think will be most predictive of the outcome
    \begin{itemize}
        \item Two categorical features: \texttt{OverallCond}, \texttt{BldgType}
        \item Ten best predictors based on correlation: \texttt{SalePrice}, 
        \texttt{OverallQual}, \texttt{GrLivArea}, \texttt{GarageCars}, 
        \texttt{GarageArea}, \texttt{TotalBsmtSF}, \texttt{1stFlrSF},
        \texttt{FullBath}, \texttt{TotRmsAbvGrd}, \texttt{YearBuilt}, 
        \texttt{YearRemodAdd}
    \end{itemize}
    
    \item Transform the categorical features in your chosen set to make them
    suitable for modeling
    
    See code below.
    
    \item Apply ridge regression to the data and find the best value of the 
    regularization parameter using cross-validation. Report the RMSE of your best 
    model both in training and validation sets. Do you see any overfitting in your 
    model?


\end{enumerate}
\pagebreak

\section{Feature Selection}
\pagebreak

\section{Predicting The Election Outcome}
\begin{lstlisting}[language=Python, caption=Code for Question 1 \emph{Hypothesis
Testing}]

\end{lstlisting}

\pagebreak

\section{Regression to the Mean}

%\bibliographystyle{plain}
%\bibliography{references}
\end{document}